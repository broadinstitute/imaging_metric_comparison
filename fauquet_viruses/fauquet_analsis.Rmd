---
title: "Viruses Dataset of Fauquet"
output: html_document
---
In order to determine if the method is well implemented. First, we test the method on the same dataset as in the paper:
[The viruses dataset of Fauquet](https://www.stats.ox.ac.uk/pub/PRNN/)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE, message=FALSE}
library(ggplot2)
library(caret)
library(magrittr)
library(dplyr)
library(tidyverse)
library(stringr)
```

## Dataset

```{r}
test <- read.table("viruses.dat", header=FALSE)
```

## Simple ranking (SR)
allows to select mc features according to the highest ranking order of their CE values

# Based on eigenvalues of matrix AAt

```{r function for entropy}
#' Function that calculate the entropy based on SVD.
#'
#' @param A dataset mxn (m features and n observations)
#' @return entropy
calculate_entropy <- function(A){
  
  # calcualte AA.t (mxm)
  AAt <- tcrossprod(A, A)
  
  # calculate the eigenvalues
  s <- eigen(AAt)
  
  # normalize relative values
  v <- s$values/sum(s$values)
  
  # calculate the entropy
  ind.zero <- which(v <= 0)
  if(length(ind.zero)==0){
    E <- -sum(v * log(v))/log(length(v))
  }else{
    E <- -sum(v[-ind.zero] * log(v[-ind.zero]))/log(length(v))
  }
  
    
  return(E)
}

```

```{r feature selection}
start.time <- Sys.time()

# transpose the dataset to have featxobs (mxn)
A <- test %>% as.matrix() %>% t(.)

# total entropy
E <- calculate_entropy(A)

# CE is the contribution vector of each feature to the entropy
CE <- c()

# for each feature calculate the contribution to the entropy by a leave-one-out comparison
for(i in 1:dim(A)[1]){
  Ei <- calculate_entropy(A[-i,])
  CE <- c(CE, E - Ei)
}

# average of all CE
c <- mean(CE) # -0.00141627475768009
# standard deviation of all CE
d <- sd(CE)# 0.00762916252676454

# features to keep, when CEi > c + d
ind.CEi <- which(CE >= c + d) # select 387 features
# names of the features to keep
names.CEi <- rownames(A)[ind.CEi] # V2 and V6

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken # 0.0889 secs (caculate_entropy)

plot(sort(CE,decreasing = TRUE), xlab = 'AAC', ylim = c(-0.02, 0.02), ylab = 'CE', type = 'h') + abline(h = c, lty=2) + abline(h = 0, col = "red") + abline(h = c + d, lty=3)
```
## Alternative method
When the number of features is lower than the number of sample, calculating the SVD is really costly. An alternative method is to calculate the SVD of the matrix XtX which will be computationaly less expensive.
```{r alternative for svd-entropy}
#' Function that calculate the entropy based on SVD. With A=XtX instead of A=Xt
#'
#' @param A dataset mxm (m features and n observations)
#' @return entropy
calculate_svd_entropy_new <- function(A){
  
  # calculate the svd
  s <- svd(A)
  sj2 <- s$d
  # normalize relative values
  v <- sj2/sum(sj2)
  
  # calculate the entropy
  ind.zero <- which(v <= 0)
  if(length(ind.zero)==0){
    E <- -sum(v * log(v))/log(length(v))
  }else{
    E <- -sum(v[-ind.zero] * log(v[-ind.zero]))/log(length(v))
  }
  
  return(E)
}

```

```{r feature selection}
start.time <- Sys.time()

# transpose the dataset to have featxobs (mxn)
X <- test %>% as.matrix() %>% t(.)

A <- tcrossprod(X, X)

# total entropy
E <- calculate_svd_entropy_new(A)

# CE is the contribution vector of each feature to the entropy
CE <- c()

# for each feature calculate the contribution to the entropy by a leave-one-out comparison
for(i in 1:dim(A)[1]){
  Ei <- calculate_svd_entropy_new(A[-i,-i]) # since the matrix A is XtX, the feature i is present in both the i-th row and column.
  CE <- c(CE, E - Ei)
}

# average of all CE
c <- mean(CE) # -0.001416274757680083
# standard deviation of all CE
d <- sd(CE)# 0.00762016252676452

# features to keep, when CEi > c + d
ind.CEi <- which(CE >= c + d) # select 387 features
# names of the features to keep
names.CEi <- rownames(A)[ind.CEi] # V2 and V6

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken # 0.04960394 secs (caculate_entropy)

plot(sort(CE,decreasing = TRUE), xlab = 'AAC', ylim = c(-0.02, 0.02), ylab = 'CE', type = 'h') + abline(h = c, lty=2) + abline(h = 0, col = "red") + abline(h = c + d, lty=3)
```

# Based on the singular value of matrix A

```{r function for entropy with SVD}
#' Function that calculate the entropy based on SVD.
#'
#' @param A dataset mxn (m features and n observations)
#' @return entropy
calculate_svd_entropy <- function(A){
  
  # calculate the svd
  s <- svd(A)
  sj2 <- s$d ** 2
  # normalize relative values
  v <- sj2/sum(sj2)
  
  # calculate the entropy
  ind.zero <- which(v <= 0)
  if(length(ind.zero)==0){
    E <- -sum(v * log(v))/log(length(v))
  }else{
    E <- -sum(v[-ind.zero] * log(v[-ind.zero]))/log(length(v))
  }
  
    
  return(E)
}

```

```{r feature selection svd}
start.time <- Sys.time()

# transpose the dataset to have featxobs (mxn)
A <- test %>% as.matrix() %>% t(.)

# total entropy
E <- calculate_svd_entropy(A)

# CE is the contribution vector of each feature to the entropy
CE <- c()

# for each feature calculate the contribution to the entropy by a leave-one-out comparison
for(i in 1:dim(A)[1]){
  Ei <- calculate_svd_entropy(A[-i,])
  CE <- c(CE, E - Ei)
}

# average of all CE
c <- mean(CE)
# standard deviation of all CE
d <- sd(CE)

# features to keep, when CEi > c + d
ind.CEi <- which(CE >= c + d) # select 387 features
# names of the features to keep
names.CEi <- rownames(A)[ind.CEi]

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken # 0.04793882 secs

plot(sort(CE,decreasing = TRUE), xlab = 'AAC', ylim = c(-0.02, 0.02), ylab = 'CE', type = 'h') + abline(h = c, lty=2) + abline(h = 0, col = "red") + abline(h = c + d, lty=3)

AAC <- data.frame(CE = sort(CE, decreasing = TRUE), AAC = aa.names)
aa.names <- c("GLY", "THR", "LYS", "SER", "MET", "HIS", "TYR", "PHE", "TRP", "PRO", "ILE", "CYS", "ARG", "VAL","GLX", "LEU", "ALA", "ASX")

tmp.df <- data.frame(AAC = aa.names, SR = 1:18)

AAC.met <- data.frame(CE = CE)

AAC.met %<>% left_join(., AAC, by = 'CE')

AAC.met %<>% left_join(., tmp.df, by = "AAC")

```

## FS1 method
Forward selection
--> this gives the same order of the selected features as in the paper!
```{r}
# transpose the dataset to have featxobs (mxn)
A <- test %>% as.matrix() %>% t(.)

# calculate the total entropy
E <- calculate_svd_entropy(A)

# CE is the contribution vector of each feature to the entropy
CE <- c()

# for each feature calculate the contribution to the entropy by a leave-one-out comparison
for(i in 1:dim(A)[1]){
  Ei <- calculate_svd_entropy(A[-i,])
  CE <- c(CE, E - Ei)
}

# select the best feature
idx.best <- which.max(CE)
E.prev <- CE[idx.best]

new.A <- A[idx.best,]
A <- A[-idx.best,]
idx.best.save <- idx.best
for(j in 1:(dim(A)[1]-1)){
  CE <- c()
  # for all features
  for(a in 1:dim(A)[1]){
    Ei <- calculate_svd_entropy(rbind(new.A, A[a,]))
    CE <- c(CE, Ei - E.prev)
  }
  idx <- which.max(CE)
  idx.best.save <- c(idx.best.save, idx)
  E.prev <- CE[idx]
  new.A <- rbind(new.A, A[idx,])
  A <- A[-idx,]
}

# final order: GLY, THR, LYS, MET, TRP, HIS, PHE, TYR, CYS, ILE, PRO, ARG, SER, VAL, LEU, GLX, ALA, ASX

```

## FS2 method
Forward selection
--> this gives the same order of the selected features as in the paper!
```{r}
# transpose the dataset to have featxobs (mxn)
A <- test %>% as.matrix() %>% t(.)

idx.best <- c()

for(j in 1:(dim(A)[1]-2)){
  # total entropy
  E <- calculate_svd_entropy(A)
  
  # CE is the contribution vector of each feature to the entropy
  CE <- c()
  
  # for each feature calculate the contribution to the entropy by a leave-one-out comparison
  for(i in 1:dim(A)[1]){
    Ei <- calculate_svd_entropy(A[-i,])
    CE <- c(CE, E - Ei)
  }
  idx <- which.max(CE)
  idx.best <- c(idx.best, idx)
  A <- A[-idx,]
}

# final order: GLY, THR, LYS, SER, PHE, PRO, HIS, VAL, GLX, ARG, ...

```


## BE method
Forward selection
--> this gives the same order of the selected features as in the paper!
```{r}
# transpose the dataset to have featxobs (mxn)
A <- test %>% as.matrix() %>% t(.)

idx.best <- c()

for(j in 1:(dim(A)[1]-2)){
  # total entropy
  E <- calculate_svd_entropy(A)
  
  # CE is the contribution vector of each feature to the entropy
  CE <- c()
  
  # for each feature calculate the contribution to the entropy by a leave-one-out comparison
  for(i in 1:dim(A)[1]){
    Ei <- calculate_svd_entropy(A[-i,])
    CE <- c(CE, E - Ei)
  }
  idx <- which.min(CE)
  idx.best <- c(idx.best, idx)
  A <- A[-idx,]
}

# final order: GLY, THR, LYS, MET, TRP, HIS, PHE, TYR, CYS, ILE, PRO, ARG, SER, VAL, LEU, GLX, ALA, ASX

```





