---
title: "Replicate Distance Jaccard"
output:
  html_document: default
---
The goal is to do some Hit Selection. Selecting the compounds that have effects, that are showing a phenotype.
In order to do so, lets find the median replicate distance. 
The smaller the distance, the more correlated the replicates are.

The distance is defined following the Jaccard distance, which calculate the dissimilarities between sample sets.
The distance is defined as the mean of the distance between 4 sets. 

A) n top features of component x

B) n top features of component y

C) n bottom features of component x

D) n bottom features of component y

$dist(x,y) = \frac{dist_J(A,B) + dist_J(C,D)}{2}$

$dist_J(i,j) = \frac{|A \cup B| - |A \cap B|}{|A \cup B|}$ 

where n is a parameter to be defined, representing the number of features in a particular set (A, B, C or D)

## Data

The input data is a 7680 by 803 matrix.
There are 7680 different observations and 799 features (extracted with CellProfiler).
Each compound (1600 different) has 4 replicates. The negative control has 1280 replicates.
20 plates with 384 wells in each plate.
selected.features contains the features that were selected in featureSelection.Rmd (using findCorrelation), the data is a 6400 by 270 matrix (negative control having been removed). 

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
# all usefull libraries
library(magrittr)
library(dplyr)
library(ggplot2)
library(foreach)
library(doMC)
library(stringr)
library(tidyverse)
```


```{r import data with feature selection, message=FALSE}
profiles <- 
  readr::read_csv(file.path("..", "..", "input", "BBBC022_2013", "BBBC022_2013_sel_feat_wrong_trt.csv")) # 7680x(nfeat+metadata)

variables <-
  names(profiles) %>% str_subset("^Cells_|^Cytoplasm_|^Nuclei_") # nfeat

metadata <-
  names(profiles) %>% str_subset("^Metadata_") # metadata

# Remove the negative control (DMSO) from the data
profiles %<>% 
  filter(!Metadata_broad_sample %in% "DMSO") # 6400xnfeat
```

```{r import data no feature selection, message=FALSE, eval=FALSE}
profiles <- 
  list.files("../../backend/BBBC022_2013/", 
          pattern = "*_normalized.csv",
          recursive = T,
          full.names = T) %>%
  map_df(read_csv)

dim(profiles)

variables <-
  names(profiles) %>% str_subset("^Cells_|^Cytoplasm_|^Nuclei_")

metadata <-
  names(profiles) %>% str_subset("^Metadata_")
  
profiles %<>%
  cytominer::select(
    sample = 
      profiles %>% 
      filter(Metadata_pert_type == "control"),
    variables = variables,
    operation = "variance_threshold"
  )

variables <-
  names(profiles) %>% str_subset("^Cells_|^Cytoplasm_|^Nuclei_")

# Remove the negative control (DMSO) from the data
profiles %<>% 
  filter(!Metadata_broad_sample %in% "DMSO") # 6400xnfeat

```


## Parameters
```{r parameters}
# number of data to make the non replicate correlation
N <- 5000

# seed for the reproducibility
set.seed(42)

# number of CPU cores for parallelization
registerDoMC(7)

# number of features to take
n.feat <- 30
```

## Separation of data

Separation is made according to the compound that was added.
Metadata_broad_sample = gives the ID of the compound that was added.

```{r separation of data}
# find the different compounds
IDs <- distinct(profiles, Metadata_broad_sample)
dim(IDs)
```

## Distance of the data

Calculate the distance of the replicate for each compound.
With features selection, computation time goes from ~15 minutes (~ 4 mins with parallelization ~ 25 secs with as.matrix) to ~5 minutes (~ 1.4 with parallelization ~ 10 seconds with as.matrix).

```{r distance in parallel}
start.time <- Sys.time()

# function to calculate the distance between two sets
distJaccard <- function(x, y){
  # 4 sets: n.feat first and n.feat lasts features
  x.sort <- sort(x)
  y.sort <- sort(y)
  
  A <- names(x.sort[1:n.feat])
  B <- names(y.sort[1:n.feat])
  
  # sorting each time
  C <- names(x.sort[seq.int(to = length(x.sort), length.out = n.feat)])
  D <- names(y.sort[seq.int(to = length(y.sort), length.out = n.feat)])
  
  d.top <- 
    (length(union(A, B)) - length(intersect(A, B)))/length(union(A, B))
  d.bottom <-
    (length(union(C, D)) - length(intersect(C, D)))/length(union(C, D))
    
  return((d.top + d.bottom)/2)
}


# loop over all IDs and save the median of the distance
comp.dist.median <- foreach(i = 1:length(IDs$Metadata_broad_sample), .combine=cbind) %dopar% {

  #filtering to choose only for one compound
  comp <-
    filter(profiles, Metadata_broad_sample %in% IDs$Metadata_broad_sample[i])
  comp <-
    comp[, variables] %>%
    as.matrix()

  # distance of the features
  comp.dist <-
    lapply(1:dim(comp)[1], function(x) (lapply(1:dim(comp)[1], function(y) return(distJaccard(comp[x,], comp[y,]))) %>%
                                          unlist)) %>%
    do.call(rbind, .)
  
  # save median of the correlation
  comp.dist.median <- 
    median(comp.dist[lower.tri(comp.dist)],na.rm=TRUE)

}

hist(comp.dist.median,
     main="Histogram for Median Replicate Distance",
     xlab="Median Replicate Distance", 
     xlim = range(0, 1))

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken # 1.4 minutes
```


## Thresholding of poor replicate distance

H0: median non replicate distance

The Null distribution is estimated by finding the median distance of non replicates. 
Select randomly 4 replicates each coming from a different compound and calculate the median distance. 
Repeat this N times to get a distribution.
Finally estimate a threshold (5th percentile) to filter out compounds with poor replicate distance.

```{r non replicate distance parallel}
start.time <- Sys.time()

# set seed for reproducibility
set.seed(42)

# random sequence for reproducibility
a <- sample(1:10000, N, replace=F)

# loop over N times to get a distribution
random.replicate.dist.median <- foreach(i = 1:N, .combine=cbind) %dopar% {
  # set seed according to random sequence
  set.seed(a[i])
  
  # group by IDs
  # sample fixed number per group -> choose 4 replicates randomly from different group
  random.replicate <- 
    profiles %>% 
    group_by(Metadata_broad_sample) %>% 
    sample_n(1, replace = FALSE) %>% 
    ungroup(random.replicate)
  random.replicate <- sample_n(random.replicate, 4, replace = FALSE)
  
  
  comp <- random.replicate[,variables] %>% 
    as.matrix()
  
  # distance of the features
  comp.dist <- 
    lapply(1:dim(comp)[1], function(x) (lapply(1:dim(comp)[1], function(y) return(distJaccard(comp[x,], comp[y,]))) %>%
                                                     unlist)) %>% 
    do.call(rbind, .)
  
  # median of the non replicate distance
  random.replicate.dist.median <- median(comp.dist[lower.tri(comp.dist)],na.rm=TRUE)
  
}

# histogram plot
hist(random.replicate.dist.median,
     main="Histogram for Non Replicate Median Distance",
     xlab="Non Replicate Median Distance", 
     xlim = range(0, 1))

# threshold to determine if can reject H0
thres <- quantile(random.replicate.dist.median, .05)
print(thres)

end.time <- Sys.time() # 2.95 mins (without feature selection)
time.taken <- end.time - start.time
time.taken
```

## Hit Selection

Select strong replicate.

```{r Hit Selection}
# find indices of replicate median distance < threshold
inds <- which(comp.dist.median < thres)

# find values of the median that are hit selected
hit.select <- comp.dist.median[inds]

# find component that are hit selected
hit.select.IDs <- IDs$Metadata_broad_sample[inds]

# ratio of strong median replicate correlation
high.median.dist <- length(hit.select)/length(comp.dist.median)
print(high.median.dist)

```


## Results

|  Method   | Pearson | Spearman | Kendall | Euclidean | Maximum | Manhattan | Distance based on Jaccard | Pearson with feat. sel |
| --------- | ------- | -------- | ------- | --------- | ------- | --------- | ------------------------- | ---------------------- |
| N = 1000  | 0.5469  |          |         |           |         |           |                           |                        |
| N = 3000  | 0.5369  |          |         |           |         |           |                           |                        |
| N = 5000  | 0.5287  | 0.4906   | 0.4944  | 0.34      | 0.2793  | 0.3525    | ~ 0.55                    | 0.5744                 |
| N = 8000  | 0.5300  |          |         |           |         |           |                           |                        |
| N = 10000 | 0.5281  |          |         |           |         |           |                           |                        |


- Difference between Pearson and Spearman correlation seem not to be very significant (no statistical test was performed).
- Distance method compared to correlation metric gives a lower ratio of hit selection (more or less 10% lower).
- Distance based on Jaccard give the best result: but computationally more costly. Take ~10min for the replicates part and ~40min for the non replicates part (for N = 5000)


| Dist. with Jaccard  | sample size N = 5000                  |
| ------------------- | ------------------------------------- |
| without feat. sel.  | 0.5063  (nb of features in set = 100) |
| without feat. sel.  | 0.5456   (nb of features in set = 50) |
| without feat. sel.  | 0.5638   (nb of features in set = 30) |
| without feat. sel.  | 0.5463   (nb of features in set = 10) |
| with feat. sel.     | 0.5375   (nb of features in set = 50) |
| with feat. sel.     | 0.5413   (nb of features in set = 40) |
| with feat. sel.     | 0.5269   (nb of features in set = 30) |
| with feat. sel.     | 0.5368   (nb of features in set = 20) |
| with feat. sel.     | 0.5381   (nb of features in set = 10) |

## Saving data

```{r output data}
# select high median correlation replicate 
profiles %<>% 
  filter(Metadata_broad_sample %in% hit.select.IDs)

profiles %>%
  write_csv("../../input/BBBC022_2013/Hit_Jaccard_feat_sel_trt.csv")

```